{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:    # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dictionary of: (i, j): r, or: (row, col): reward\n",
    "        # actions should be a dictionary of: (i, j): A or (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        # The state s is the location of the player in the grid: s = (i, j)\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        # A terminal state won't be in the actions dictionary (since it won't have any associated action)\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        # Possible actions: U/D/L/R\n",
    "        if action in self.actions[self.current_state()]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return reward (if any)\n",
    "        return self.rewards.get(self.current_state(), 0)\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "        \n",
    "    def game_over(self):\n",
    "        # The game is over if we are in a state where no action is possible\n",
    "        return self.current_state() not in self.actions\n",
    "    \n",
    "    def all_states(self):\n",
    "        # Cast to a set to avoid repetition in states\n",
    "        return set(list(self.rewards.keys()) + list(self.actions.keys()))\n",
    "    \n",
    "    def draw_grid(self):\n",
    "        states = self.all_states()\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                s = (i, j)\n",
    "                symbol = ''\n",
    "                if s in states:\n",
    "                    if self.current_state() == s:\n",
    "                        symbol = 's'\n",
    "                    else:\n",
    "                        symbol = '.'\n",
    "                else:\n",
    "                    symbol = 'x'\n",
    "                print(symbol, end = '')\n",
    "                if j != self.width - 1:\n",
    "                    print('   ', end = '')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_grid():\n",
    "    # Define a grid that describes the reward for arriving at each state\n",
    "    #     and possible actions at each state\n",
    "    # The grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .   .   .   1\n",
    "    # .   x   .  -1\n",
    "    # s   .   .   .\n",
    "    g = Grid(4, 3, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0,0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('D', 'L', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('U', 'L', 'R'),\n",
    "        (2, 3): ('U', 'L')\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_ENOUGH = 10e-4    # threshold for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V is the value function dictionary, and g is the grid (environment)\n",
    "def print_values(V, g):\n",
    "    for i in range(g.height):\n",
    "        print('-------------------------')\n",
    "        print('|', end = \"\")\n",
    "        for j in range(g.width):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end = \"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end = \"\")    # negative sign takes up an extra space\n",
    "        print()\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P is the policy dictionary (Mapping each state to the action to take)\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.height):\n",
    "        print('-------------------------')\n",
    "        print('|', end = '')\n",
    "        for j in range(g.width):\n",
    "            a = P.get((i, j), ' ')\n",
    "            print('  %s  |' % a, end = '')\n",
    "        print()\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = standard_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = grid.all_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iterative Policy Evaluation\n",
    "### Policy: Uniformely random actions\n",
    "### In this case, the probability of performing action a in state s is 1 / len(grid.actions[s])\n",
    "### The action state transitions are still deterministic (if you are in state s and you perform action a, you only end up in one possible state s')\n",
    "\n",
    "# initialize V(s) = 0\n",
    "V = {}\n",
    "for s in states:\n",
    "    V[s] = 0\n",
    "gamma = 1.0\n",
    "\n",
    "while True:\n",
    "    delta = 0    # max change in value function\n",
    "    for s in states:\n",
    "        if not grid.is_terminal(s):\n",
    "            grid.set_state(s)\n",
    "            old_v = V[s]\n",
    "            V[s] = 0\n",
    "            prob_a_s = 1 / len(grid.actions[s])\n",
    "            for action in grid.actions[s]:\n",
    "                # perform action to get the reward and the next state's value, then undo action\n",
    "                r = grid.move(action)\n",
    "                v_s_prime = V[grid.current_state()]\n",
    "                V[s] += r + gamma * v_s_prime\n",
    "                grid.undo_move(action)\n",
    "            V[s] *= prob_a_s\n",
    "            delta = max(delta, abs(V[s] - old_v))\n",
    "    if delta < SMALL_ENOUGH:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|-0.03| 0.09| 0.22| 0.00|\n",
      "-------------------------\n",
      "|-0.16| 0.00|-0.44| 0.00|\n",
      "-------------------------\n",
      "|-0.29|-0.41|-0.54|-0.77|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.current_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".   .   s   .\n",
      "\n",
      ".   x   .   .\n",
      "\n",
      ".   .   .   .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid.draw_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {\n",
    "    (0,0):'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2):'R',\n",
    "    (1, 0):'U',\n",
    "    (1, 2):'R',\n",
    "    (2, 0):'U',\n",
    "    (2, 1):'R',\n",
    "    (2, 2):'R',\n",
    "    (2, 3):'U'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iterative Policy Evaluation\n",
    "### Here the policy is given (deterministic)\n",
    "### The action state transitions are still deterministic (if you are in state s and you perform action a, you only end up in one possible state s')\n",
    "\n",
    "# initialize V(s) = 0\n",
    "V2 = {}\n",
    "for s in states:\n",
    "    V2[s] = 0\n",
    "gamma = 0.9\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    for s in states:\n",
    "        if not grid.is_terminal(s):\n",
    "            grid.set_state(s)\n",
    "            old_v = V2[s]\n",
    "            # In this case, there is only one action per state, so no need for a for loop\n",
    "            action = policy[s]\n",
    "            r = grid.move(action)\n",
    "            v_s_prime = V2[grid.current_state()]\n",
    "            V2[s] = r + gamma * v_s_prime\n",
    "            grid.undo_move(action)\n",
    "            \n",
    "            delta = max(delta, abs(V2[s] - old_v))\n",
    "    if delta < SMALL_ENOUGH:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "| 0.81| 0.90| 1.00| 0.00|\n",
      "-------------------------\n",
      "| 0.73| 0.00|-1.00| 0.00|\n",
      "-------------------------\n",
      "| 0.66|-0.81|-0.90|-1.00|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V2, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|  R  |  R  |  R  |     |\n",
      "-------------------------\n",
      "|  U  |     |  R  |     |\n",
      "-------------------------\n",
      "|  U  |  R  |  R  |  U  |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_grid():\n",
    "    # Define a grid that describes the reward for arriving at each state\n",
    "    #     and possible actions at each state\n",
    "    # The grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .   .   .   1\n",
    "    # .   x   .  -1\n",
    "    # s   .   .   .\n",
    "    g = Grid(4, 3, (2, 0))\n",
    "    actions = {\n",
    "        (0,0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('D', 'L', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('U', 'L', 'R'),\n",
    "        (2, 3): ('U', 'L')\n",
    "    }\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    \n",
    "    # Penalise the player for each step, to see if he can finish with the minimum step\n",
    "    for s in actions.keys():\n",
    "        rewards[s] = -0.1\n",
    "    g.set(rewards, actions)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = negative_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|-0.10|-0.10|-0.10| 1.00|\n",
      "-------------------------\n",
      "|-0.10| 0.00|-0.10|-1.00|\n",
      "-------------------------\n",
      "|-0.10|-0.10|-0.10|-0.10|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(g.rewards, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS = ['U', 'D', 'L', 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value function corresponding to the given policy\n",
    "### Iterative Policy Evaluation\n",
    "### Here the policy is given (deterministic)\n",
    "### The action state transitions are still deterministic (if you are in state s and you perform action a, you only end up in one possible state s')\n",
    "def get_value_function(policy, g):\n",
    "    states = g.all_states()\n",
    "    # initialize V(s) = 0\n",
    "    V3 = {}\n",
    "    for s in states:\n",
    "        V3[s] = 0\n",
    "    gamma = 0.9\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if not g.is_terminal(s):\n",
    "                g.set_state(s)\n",
    "                old_v = V3[s]\n",
    "                action = policy[s]\n",
    "                r = g.move(action)\n",
    "                v_s_prime = V3[g.current_state()]\n",
    "                V3[s] = r + gamma * v_s_prime\n",
    "                g.undo_move(action)\n",
    "                \n",
    "                delta = max(delta, abs(V3[s] - old_v))\n",
    "        if delta < SMALL_ENOUGH:\n",
    "            break\n",
    "    return V3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|  R  |  L  |  L  |     |\n",
      "-------------------------\n",
      "|  D  |     |  R  |     |\n",
      "-------------------------\n",
      "|  R  |  R  |  U  |  L  |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(policy, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|-1.00|-1.00|-1.00| 0.00|\n",
      "-------------------------\n",
      "|-1.00| 0.00|-1.00| 0.00|\n",
      "-------------------------\n",
      "|-1.00|-1.00|-1.00|-1.00|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "V3 = get_value_function(policy, g)\n",
    "print_values(V3, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Policy Iteration Algorithm\n",
    "### Finds the optimal policy, starting with a random policy and ameliorating it\n",
    "def policy_iteration(g):\n",
    "    policy = {}\n",
    "    \n",
    "    # Randomly initialize policy\n",
    "    # Since the policy is a state -> action mapping, randomly initializing the policy is randomly choosing an action for each non terminal state\n",
    "    for s in g.actions.keys():\n",
    "        policy[s] = np.random.choice(g.actions[s])\n",
    "        \n",
    "        \n",
    "    policy_changed = True\n",
    "        \n",
    "    # Keep updating the policy (by finding better actions for each state) until the policy doesn't change anymore\n",
    "    while policy_changed:\n",
    "        V3 = get_value_function(policy, g)\n",
    "        policy_changed = False\n",
    "        gamma = 0.9\n",
    "        states = g.all_states()\n",
    "        for s in states:\n",
    "            if not g.is_terminal(s):\n",
    "                g.set_state(s)\n",
    "                old_a = policy[s]    # Old action determined by the old policy\n",
    "                # Find the best action (action with highest value)\n",
    "                max_a = old_a\n",
    "                max_val = float('-inf')\n",
    "                for a in g.actions[s]:\n",
    "                    # Perform action, get value, then undo action\n",
    "                    r = g.move(a)\n",
    "                    val = r + gamma * V3[g.current_state()]\n",
    "                    if val > max_val:\n",
    "                        max_a = a\n",
    "                        max_val = val\n",
    "                    g.undo_move(a)\n",
    "                policy[s] = max_a\n",
    "                # if the new action is different from the one deyermined by the old policy, then the policy has changed\n",
    "                if policy[s] != old_a:\n",
    "                    policy_changed = True\n",
    "    return policy, V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_policy, opt_val_func = policy_iteration(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|  R  |  R  |  R  |     |\n",
      "-------------------------\n",
      "|  U  |     |  U  |     |\n",
      "-------------------------\n",
      "|  U  |  R  |  U  |  L  |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(opt_policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "| 0.62| 0.80| 1.00| 0.00|\n",
      "-------------------------\n",
      "| 0.46| 0.00| 0.80| 0.00|\n",
      "-------------------------\n",
      "| 0.31| 0.46| 0.62| 0.46|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(opt_val_func, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
