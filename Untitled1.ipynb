{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:    # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dictionary of: (i, j): r, or: (row, col): reward\n",
    "        # actions should be a dictionary of: (i, j): A or (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        # The state s is the location of the player in the grid: s = (i, j)\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        # A terminal state won't be in the actions dictionary (since it won't have any associated action)\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        # Possible actions: U/D/L/R\n",
    "        if action in self.actions[self.current_state()]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return reward (if any)\n",
    "        return self.rewards.get(self.current_state(), 0)\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "        \n",
    "    def game_over(self):\n",
    "        # The game is over if we are in a state where no action is possible\n",
    "        return self.current_state() not in self.actions\n",
    "    \n",
    "    def all_states(self):\n",
    "        # Cast to a set to avoid repetition in states\n",
    "        return set(list(self.rewards.keys()) + list(self.actions.keys()))\n",
    "    \n",
    "    def draw_grid(self):\n",
    "        states = self.all_states()\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                s = (i, j)\n",
    "                symbol = ''\n",
    "                if s in states:\n",
    "                    if self.current_state() == s:\n",
    "                        symbol = 's'\n",
    "                    else:\n",
    "                        symbol = '.'\n",
    "                else:\n",
    "                    symbol = 'x'\n",
    "                print(symbol, end = '')\n",
    "                if j != self.width - 1:\n",
    "                    print('   ', end = '')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_negative_grid():\n",
    "    # Define a grid that describes the reward for arriving at each state\n",
    "    #     and possible actions at each state\n",
    "    # The grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .   .   .   1\n",
    "    # .   x   .  -1\n",
    "    # s   .   .   .\n",
    "    g = Grid(4, 3, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0,0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('D', 'L', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('U', 'L', 'R'),\n",
    "        (2, 3): ('U', 'L')\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    for s in g.all_states():\n",
    "        if not g.is_terminal(s):\n",
    "            g.rewards[s] = -0.1\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_ENOUGH = 10e-4    # threshold for convergence\n",
    "GAMMA = 0.9    # discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V is the value function dictionary, and g is the grid (environment)\n",
    "def print_values(V, g):\n",
    "    for i in range(g.height):\n",
    "        print('-------------------------')\n",
    "        print('|', end = \"\")\n",
    "        for j in range(g.width):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end = \"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end = \"\")    # negative sign takes up an extra space\n",
    "        print()\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P is the policy dictionary (Mapping each state to the action to take)\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.height):\n",
    "        print('-------------------------')\n",
    "        print('|', end = '')\n",
    "        for j in range(g.width):\n",
    "            a = P.get((i, j), ' ')\n",
    "            print('  %s  |' % a, end = '')\n",
    "        print()\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_grid():\n",
    "    # Define a grid that describes the reward for arriving at each state\n",
    "    #     and possible actions at each state\n",
    "    # The grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .   .   .   1\n",
    "    # .   x   .  -1\n",
    "    # s   .   .   .\n",
    "    g = Grid(4, 3, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0,0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('D', 'L', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('U', 'L', 'R'),\n",
    "        (2, 3): ('U', 'L')\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    for s in g.all_states():\n",
    "        if not g.is_terminal(s):\n",
    "            g.rewards[s] = 0\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(grid, policy, start = (None, None)):\n",
    "    unecessary_action = False\n",
    "    states_actions_rewards = []\n",
    "    traversed_states = []\n",
    "    if start[0] is None:\n",
    "        # Reset game to start at random position\n",
    "        start_states = list(grid.actions.keys())\n",
    "        start_idx = np.random.choice(len(start_states))\n",
    "        grid.set_state(start_states[start_idx])\n",
    "    else:\n",
    "        grid.set_state(start[0])\n",
    "    \n",
    "    s = grid.current_state()\n",
    "    traversed_states.append(s)\n",
    "    #grid.draw_grid()\n",
    "    if start[1] is not None:\n",
    "        a = start[1]\n",
    "        old_state = s\n",
    "        #print('action:', a)\n",
    "        grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        if s in traversed_states:\n",
    "            states_actions_rewards.append((old_state, a, -100))\n",
    "            unecessary_action = True\n",
    "        else:\n",
    "            # Save state and reward obtained\n",
    "            states_actions_rewards.append((old_state, a, grid.rewards[s]))\n",
    "    \n",
    "    \n",
    "    s = grid.current_state()    # starting position\n",
    "    while not unecessary_action and not grid.game_over():\n",
    "        traversed_states.append(s)\n",
    "        # Get action to take from policy, then perform action\n",
    "        a = policy[s]\n",
    "        #print('action:', a)\n",
    "        old_state = s\n",
    "        grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        if s in traversed_states:\n",
    "            states_actions_rewards.append((old_state, a, -100))\n",
    "            unecessary_action = True\n",
    "        else:\n",
    "            # Save state and reward obtained\n",
    "            r = grid.rewards[s]\n",
    "            states_actions_rewards.append((old_state, a, r))\n",
    "        #grid.draw_grid()\n",
    "    \n",
    "    if not unecessary_action:\n",
    "        states_actions_rewards.append((s, None, 0))\n",
    "    #print('game over')\n",
    "    #print('----------------------------------------')\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    #print(states_actions_rewards)\n",
    "    # Easier to compute returns when reversed\n",
    "    states_actions_rewards.reverse()\n",
    "    for s, a, r in states_actions_rewards:\n",
    "        G = r + GAMMA * G\n",
    "        states_actions_returns.append((s, a, G))\n",
    "    # reverse back for order\n",
    "    states_actions_returns.reverse()\n",
    "    \n",
    "    return states_actions_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is the number of episodes\n",
    "def first_visit_monte_carlo_prediction(grid, policy, N):\n",
    "    states = grid.all_states()\n",
    "    V = {}\n",
    "    counts = {}\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "        counts[s] = 0    # Keeps track of the number of times a state is visited\n",
    "    for i in range(1, N + 1):\n",
    "        states_actions_returns = play_game(grid, policy)\n",
    "        seen_states = set()\n",
    "        #print(states_actions_returns)\n",
    "        for j in range(len(states_actions_returns)):\n",
    "            s, a, g = states_actions_returns[j]\n",
    "            # Check states that were seen before to see if\n",
    "            #  the current state was seen already\n",
    "            if not s in seen_states:\n",
    "                counts[s] += 1\n",
    "                # Efficient way to calculate the mean\n",
    "                V[s] = (1 - 1 / counts[s]) * V[s] + (1 / counts[s]) * g          \n",
    "                seen_states.add(s)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = standard_grid()\n",
    "policy = {\n",
    "    (0,0):'R',\n",
    "    (0, 1): 'L',\n",
    "    (0, 2):'R',\n",
    "    (1, 0):'U',\n",
    "    (1, 2):'R',\n",
    "    (2, 0):'U',\n",
    "    (2, 1):'R',\n",
    "    (2, 2):'R',\n",
    "    (2, 3):'U'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|  R  |  L  |  R  |     |\n",
      "-------------------------\n",
      "|  U  |     |  R  |     |\n",
      "-------------------------\n",
      "|  U  |  R  |  R  |  U  |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(policy, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = first_visit_monte_carlo_prediction(g, policy, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|-92.67|-97.33| 1.00| 0.00|\n",
      "-------------------------\n",
      "|-81.00| 0.00|-1.00| 0.00|\n",
      "-------------------------\n",
      "|-72.90|-0.81|-0.90|-1.00|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2, 0), 'U', -72.9),\n",
       " ((1, 0), 'U', -81.0),\n",
       " ((0, 0), 'R', -90.0),\n",
       " ((0, 1), 'L', -100.0)]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = standard_grid()\n",
    "play_game(g, policy, start = ((2, 0), 'U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGrid:    # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dictionary of: (i, j): r, or: (row, col): reward\n",
    "        # actions should be a dictionary of: (i, j): A or (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        # The state s is the location of the player in the grid: s = (i, j)\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        # A terminal state won't be in the actions dictionary (since it won't have any associated action)\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        # Possible actions: U/D/L/R\n",
    "        if action in self.actions[self.current_state()]:\n",
    "            r = np.random.random()\n",
    "            remaining_actions = list(self.actions[self.current_state()])\n",
    "            # if r > 0.5, use the given action, else chose a different one\n",
    "            if r < 0.5:\n",
    "                p = 0.5 / 3    # prob of any other action occuring\n",
    "                remaining_actions.remove(action)   # if r < 0.5, then the given action will not be performed\n",
    "                n = len(remaining_actions)\n",
    "                action_changed = False    # if it stays false, then the action we are attempting to choose is illegal, so do nothing\n",
    "                for i in range(n):\n",
    "                    if i * p <= r < (i + 1) * p:\n",
    "                        action = remaining_actions[i]\n",
    "                        action_changed = True\n",
    "                        break\n",
    "                if not action_changed:\n",
    "                    action = ''\n",
    "            \n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        return self.rewards.get(self.current_state(), 0), action\n",
    "                \n",
    "    def move_without_randomness(self, action):\n",
    "        # check if legal move first\n",
    "        # Possible actions: U/D/L/R\n",
    "        if action in self.actions[self.current_state()]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return reward (if any)\n",
    "        return self.rewards.get(self.current_state(), 0)\n",
    "               \n",
    "        # return action selected and reward (if any)\n",
    "        return action, self.rewards.get(self.current_state(), 0)\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "        \n",
    "    def game_over(self):\n",
    "        # The game is over if we are in a state where no action is possible\n",
    "        return self.current_state() not in self.actions\n",
    "    \n",
    "    def all_states(self):\n",
    "        # Cast to a set to avoid repetition in states\n",
    "        return set(list(self.rewards.keys()) + list(self.actions.keys()))\n",
    "    \n",
    "    def draw_grid(self):\n",
    "        states = self.all_states()\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                s = (i, j)\n",
    "                symbol = ''\n",
    "                if s in states:\n",
    "                    if self.current_state() == s:\n",
    "                        symbol = 's'\n",
    "                    else:\n",
    "                        symbol = '.'\n",
    "                else:\n",
    "                    symbol = 'x'\n",
    "                print(symbol, end = '')\n",
    "                if j != self.width - 1:\n",
    "                    print('   ', end = '')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_random_grid():\n",
    "    # Define a grid that describes the reward for arriving at each state\n",
    "    #     and possible actions at each state\n",
    "    # The grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .   .   .   1\n",
    "    # .   x   .  -1\n",
    "    # s   .   .   .\n",
    "    g = RandomGrid(4, 3, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0,0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('D', 'L', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('U', 'L', 'R'),\n",
    "        (2, 3): ('U', 'L')\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    for s in g.all_states():\n",
    "        if not g.is_terminal(s):\n",
    "            g.rewards[s] = 0\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_random_grid():\n",
    "    # Define a grid that describes the reward for arriving at each state\n",
    "    #     and possible actions at each state\n",
    "    # The grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .   .   .   1\n",
    "    # .   x   .  -1\n",
    "    # s   .   .   .\n",
    "    g = RandomGrid(4, 3, (2, 0))\n",
    "    actions = {\n",
    "        (0,0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('D', 'L', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('U', 'L', 'R'),\n",
    "        (2, 3): ('U', 'L')\n",
    "    }\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    \n",
    "    # Penalise the player for each step, to see if he can finish with the minimum step\n",
    "    for s in actions.keys():\n",
    "        rewards[s] = -0.1\n",
    "    g.set(rewards, actions)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = standard_random_grid()\n",
    "winning_policy = {\n",
    "    (0,0):'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2):'R',\n",
    "    (1, 0):'U',\n",
    "    (1, 2):'U',\n",
    "    (2, 0):'U',\n",
    "    (2, 1):'L',\n",
    "    (2, 2):'U',\n",
    "    (2, 3):'L'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|  R  |  R  |  R  |     |\n",
      "-------------------------\n",
      "|  U  |     |  U  |     |\n",
      "-------------------------\n",
      "|  U  |  L  |  U  |  L  |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(winning_policy, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = first_visit_monte_carlo_prediction(g, winning_policy, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|-70.49|-56.96|-29.47| 0.00|\n",
      "-------------------------\n",
      "|-76.45| 0.00|-36.14| 0.00|\n",
      "-------------------------\n",
      "|-78.53|-73.81|-52.19|-58.67|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|  R  |  R  |  R  |     |\n",
      "-------------------------\n",
      "|  U  |     |  U  |     |\n",
      "-------------------------\n",
      "|  U  |  L  |  U  |  L  |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(winning_policy, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_POSSIBLE_ACTIONS = ['U', 'D', 'L', 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(grid, Q):\n",
    "    policy = {}\n",
    "    max_values_dict = {}\n",
    "    for s, a in Q.keys():\n",
    "        if s not in max_values_dict:\n",
    "            if not grid.is_terminal(s):\n",
    "                max_values_dict[s] = Q[(s, a)]\n",
    "                policy[s] = a\n",
    "        else:\n",
    "            if Q[(s, a)] > max_values_dict[s]:\n",
    "                max_values_dict[s] = Q[(s, a)]\n",
    "                policy[s] = a\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_function(grid, Q):\n",
    "    V = {}\n",
    "    for s, a in Q.keys():\n",
    "        if s not in V:\n",
    "            if not grid.is_terminal(s):\n",
    "                V[s] = Q[(s, a)]\n",
    "        else:\n",
    "            if Q[(s, a)] > V[s]:\n",
    "                V[s] = Q[(s, a)]\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_control(grid, N):\n",
    "    Q = {}\n",
    "    policy = {}\n",
    "    states = grid.all_states()\n",
    "    non_terminal_states = list(grid.actions.keys())\n",
    "    counts = {}\n",
    "    \n",
    "    # Initialize Q and policy randomly\n",
    "    for s in states:\n",
    "        if not grid.is_terminal(s):\n",
    "            # Initialize Q\n",
    "            for a in ALL_POSSIBLE_ACTIONS:\n",
    "                Q[(s, a)] = np.random.random()\n",
    "                counts[(s, a)] = 0\n",
    "            # Initialize policy\n",
    "            a_idx = np.random.choice(len(ALL_POSSIBLE_ACTIONS))\n",
    "            policy[s] = ALL_POSSIBLE_ACTIONS[a_idx]\n",
    "        else:\n",
    "            Q[(s, None)] = 0\n",
    "            counts[(s, None)] = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Randomly select a state\n",
    "        state_idx = np.random.choice(len(non_terminal_states))\n",
    "        state = non_terminal_states[state_idx]\n",
    "        \n",
    "        # Randomly select an action\n",
    "        a_idx = np.random.choice(len(ALL_POSSIBLE_ACTIONS))\n",
    "        action = ALL_POSSIBLE_ACTIONS[a_idx]\n",
    "        \n",
    "        states_actions_returns = play_game(grid, policy, start = (state, action))\n",
    "        \n",
    "        # Policy iteration\n",
    "        seen_states = set()\n",
    "        for s, a, g in states_actions_returns:\n",
    "            if not s in seen_states:\n",
    "                counts[(s, a)] += 1\n",
    "                # Efficient way to calculate the mean\n",
    "                Q[(s, a)] = (1 - 1 / counts[(s, a)]) * Q[(s, a)] + (1 / counts[(s, a)]) * g          \n",
    "                seen_states.add(s)\n",
    "                \n",
    "        # Policy improvement\n",
    "        policy = get_policy(grid, Q)\n",
    "            \n",
    "    return Q, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = standard_negative_grid()\n",
    "Q, policy = monte_carlo_control(g, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|  D  |  R  |  R  |     |\n",
      "-------------------------\n",
      "|  D  |     |  R  |     |\n",
      "-------------------------\n",
      "|  R  |  R  |  U  |  U  |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(policy, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = get_value_function(g, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|-24.54|-28.22| 1.00| 0.00|\n",
      "-------------------------\n",
      "|-29.17| 0.00|-1.00| 0.00|\n",
      "-------------------------\n",
      "|-23.32|-18.43|-12.73|-1.00|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
