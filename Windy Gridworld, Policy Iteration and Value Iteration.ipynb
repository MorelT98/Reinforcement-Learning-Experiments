{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGrid:    # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dictionary of: (i, j): r, or: (row, col): reward\n",
    "        # actions should be a dictionary of: (i, j): A or (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        \n",
    "    def set_state(self, s):\n",
    "        # The state s is the location of the player in the grid: s = (i, j)\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "        \n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    def is_terminal(self, s):\n",
    "        # A terminal state won't be in the actions dictionary (since it won't have any associated action)\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def move(self, action):\n",
    "        # check if legal move first\n",
    "        # Possible actions: U/D/L/R\n",
    "        if action in self.actions[self.current_state()]:\n",
    "            r = np.random.random()\n",
    "            remaining_actions = list(self.actions[self.current_state()])\n",
    "            # if r > 0.5, use the given action, else chose a different one\n",
    "            if r < 0.5:\n",
    "                p = 0.5 / 3    # prob of any other action occuring\n",
    "                remaining_actions.remove(action)   # if r < 0.5, then the given action will not be performed\n",
    "                n = len(remaining_actions)\n",
    "                action_changed = False    # if it stays false, then the action we are attempting to choose is illegal, so do nothing\n",
    "                for i in range(n):\n",
    "                    if i * p <= r < (i + 1) * p:\n",
    "                        action = remaining_actions[i]\n",
    "                        action_changed = True\n",
    "                        break\n",
    "                if not action_changed:\n",
    "                    action = ''\n",
    "            \n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "                \n",
    "    def move_without_randomness(self, action):\n",
    "        # check if legal move first\n",
    "        # Possible actions: U/D/L/R\n",
    "        if action in self.actions[self.current_state()]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # return reward (if any)\n",
    "        return self.rewards.get(self.current_state(), 0)\n",
    "               \n",
    "        # return action selected and reward (if any)\n",
    "        return action, self.rewards.get(self.current_state(), 0)\n",
    "    \n",
    "    def undo_move(self, action):\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # raise an exception if we arrive somewhere we shouldn't be\n",
    "        # should never happen\n",
    "        assert(self.current_state() in self.all_states())\n",
    "        \n",
    "    def game_over(self):\n",
    "        # The game is over if we are in a state where no action is possible\n",
    "        return self.current_state() not in self.actions\n",
    "    \n",
    "    def all_states(self):\n",
    "        # Cast to a set to avoid repetition in states\n",
    "        return set(list(self.rewards.keys()) + list(self.actions.keys()))\n",
    "    \n",
    "    def draw_grid(self):\n",
    "        states = self.all_states()\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                s = (i, j)\n",
    "                symbol = ''\n",
    "                if s in states:\n",
    "                    if self.current_state() == s:\n",
    "                        symbol = 's'\n",
    "                    else:\n",
    "                        symbol = '.'\n",
    "                else:\n",
    "                    symbol = 'x'\n",
    "                print(symbol, end = '')\n",
    "                if j != self.width - 1:\n",
    "                    print('   ', end = '')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_random_grid():\n",
    "    # Define a grid that describes the reward for arriving at each state\n",
    "    #     and possible actions at each state\n",
    "    # The grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .   .   .   1\n",
    "    # .   x   .  -1\n",
    "    # s   .   .   .\n",
    "    g = RandomGrid(4, 3, (2, 0))\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    actions = {\n",
    "        (0,0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('D', 'L', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('U', 'L', 'R'),\n",
    "        (2, 3): ('U', 'L')\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_ENOUGH = 10e-8    # threshold for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V is the value function dictionary, and g is the grid (environment)\n",
    "def print_values(V, g):\n",
    "    for i in range(g.height):\n",
    "        print('-------------------------')\n",
    "        print('|', end = \"\")\n",
    "        for j in range(g.width):\n",
    "            v = V.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end = \"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end = \"\")    # negative sign takes up an extra space\n",
    "        print()\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P is the policy dictionary (Mapping each state to the action to take)\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.height):\n",
    "        print('-------------------------')\n",
    "        print('|', end = '')\n",
    "        for j in range(g.width):\n",
    "            a = P.get((i, j), ' ')\n",
    "            print('  %s  |' % a, end = '')\n",
    "        print()\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_random_grid():\n",
    "    # Define a grid that describes the reward for arriving at each state\n",
    "    #     and possible actions at each state\n",
    "    # The grid looks like this\n",
    "    # x means you can't go there\n",
    "    # s means start position\n",
    "    # number means reward at that state\n",
    "    # .   .   .   1\n",
    "    # .   x   .  -1\n",
    "    # s   .   .   .\n",
    "    g = RandomGrid(4, 3, (2, 0))\n",
    "    actions = {\n",
    "        (0,0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('D', 'L', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('U', 'L', 'R'),\n",
    "        (2, 3): ('U', 'L')\n",
    "    }\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    \n",
    "    # Penalise the player for each step, to see if he can finish with the minimum step\n",
    "    for s in actions.keys():\n",
    "        rewards[s] = -0.1\n",
    "    g.set(rewards, actions)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = negative_random_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_transition_prob(s, a, g):\n",
    "    if a not in g.actions[s]:\n",
    "        return {}\n",
    "    # Returns the possible future states and the probability to get to each state, given that we are at state s and we perform action a\n",
    "    prob_dict = {}\n",
    "    i, j = s[0], s[1]\n",
    "    # Find the next state if we perform action a and action a actually happens\n",
    "    if a == 'U':\n",
    "        i -= 1\n",
    "    elif a == 'D':\n",
    "        i += 1\n",
    "    elif a == 'L':\n",
    "        j -= 1\n",
    "    elif a == 'R':\n",
    "        j += 1\n",
    "    # The chances of action a actually happening are 0.5\n",
    "    prob_dict[(i, j)] = 0.5\n",
    "    # Find the next states if we perform action a but other actions happen\n",
    "    for action in g.actions[s]:\n",
    "        i, j = s[0], s[1]\n",
    "        if action != a:\n",
    "            if action == 'U':\n",
    "                i -= 1\n",
    "            elif action == 'D':\n",
    "                i += 1\n",
    "            elif action == 'L':\n",
    "                j -= 1\n",
    "            elif action == 'R':\n",
    "                j += 1\n",
    "            prob_dict[(i, j)] = 0.5 / 3\n",
    "    return prob_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value function corresponding to the given policy\n",
    "### Iterative Policy Evaluation\n",
    "### Here the policy is given (deterministic)\n",
    "### The actions are deterministic\n",
    "### The state transitions are probabilistic\n",
    "def get_value_function(policy, g):\n",
    "    states = g.all_states()\n",
    "    # initialize V(s) = 0\n",
    "    V = {}\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "    gamma = 0.9\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if not g.is_terminal(s):\n",
    "                g.set_state(s)\n",
    "                old_v = V[s]\n",
    "                action = policy[s]\n",
    "                states_prob = get_state_transition_prob(s, action, g)\n",
    "                V[s] = 0\n",
    "                for s_prime in states_prob.keys():\n",
    "                    p = states_prob[s_prime]\n",
    "                    if s_prime in g.rewards.keys():\n",
    "                        r = g.rewards[s_prime]\n",
    "                    else:\n",
    "                        r = 0\n",
    "                    V[s] += p * (r + gamma * V[s_prime])\n",
    "                delta = max(delta, abs(V[s] - old_v))\n",
    "        if delta < SMALL_ENOUGH:\n",
    "            break\n",
    "    return V\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".   .   .   .\n",
      "\n",
      ".   x   .   .\n",
      "\n",
      "s   .   .   .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.draw_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = {\n",
    "    (0,0):'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2):'R',\n",
    "    (1, 0):'U',\n",
    "    (1, 2):'R',\n",
    "    (2, 0):'U',\n",
    "    (2, 1):'R',\n",
    "    (2, 2):'R',\n",
    "    (2, 3):'U'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = get_value_function(policy, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|-0.03| 0.11| 0.40| 0.00|\n",
      "-------------------------\n",
      "|-0.11| 0.00|-0.54| 0.00|\n",
      "-------------------------\n",
      "|-0.16|-0.30|-0.48|-0.59|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|  R  |  R  |  R  |     |\n",
      "-------------------------\n",
      "|  U  |     |  R  |     |\n",
      "-------------------------\n",
      "|  U  |  R  |  R  |  U  |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(policy, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.5, (0, 2): 0.16666666666666666}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_state_transition_prob((0, 1), 'L', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Policy Iteration Algorithm\n",
    "### Finds the optimal policy, starting with a random policy and ameliorating it\n",
    "def policy_iteration(g):\n",
    "    policy = {}\n",
    "    \n",
    "    # Randomly initialize policy\n",
    "    # Since the policy is a state -> action mapping, randomly initializing the policy is randomly choosing an action for each non terminal state\n",
    "    for s in g.actions.keys():\n",
    "        policy[s] = np.random.choice(g.actions[s])\n",
    "        \n",
    "        \n",
    "    policy_changed = True\n",
    "        \n",
    "    # Keep updating the policy (by finding better actions for each state) until the policy doesn't change anymore\n",
    "    while policy_changed:\n",
    "        V = get_value_function(policy, g)\n",
    "        policy_changed = False\n",
    "        gamma = 0.9\n",
    "        states = g.all_states()\n",
    "        for s in states:\n",
    "            if not g.is_terminal(s):\n",
    "                g.set_state(s)\n",
    "                old_a = policy[s]    # Old action determined by the old policy\n",
    "                # Find the best action (action with highest value)\n",
    "                max_a = old_a\n",
    "                max_val = float('-inf')\n",
    "                for a in g.actions[s]:\n",
    "                    states_prob = get_state_transition_prob(s, a, g)\n",
    "                    val = 0\n",
    "                    # Get all next possible states and their probabilities to calculate value\n",
    "                    for s_prime in states_prob.keys():\n",
    "                        p = states_prob[s_prime]\n",
    "                        if s_prime in g.rewards.keys():\n",
    "                            r = g.rewards[s_prime]\n",
    "                        else:\n",
    "                            r = 0\n",
    "                        val += p * (r + gamma * V[s_prime])\n",
    "                    # Update max value and corresponding action\n",
    "                    if val > max_val:\n",
    "                        max_a = a\n",
    "                        max_val = val\n",
    "                policy[s] = max_a\n",
    "                # if the new action is different from the one deyermined by the old policy, then the policy has changed\n",
    "                if policy[s] != old_a:\n",
    "                    policy_changed = True\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, V = policy_iteration(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|  R  |  R  |  R  |     |\n",
      "-------------------------\n",
      "|  U  |     |  U  |     |\n",
      "-------------------------\n",
      "|  U  |  L  |  U  |  L  |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(policy, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|-0.01| 0.15| 0.48| 0.00|\n",
      "-------------------------\n",
      "|-0.09| 0.00|-0.04| 0.00|\n",
      "-------------------------\n",
      "|-0.13|-0.15|-0.17|-0.29|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(V, g):\n",
    "    # given the value function, this method finds the value function greedily\n",
    "    gamma = 0.9\n",
    "    policy = {}\n",
    "    states = g.all_states()\n",
    "    for s in states:\n",
    "        max_val = float('-inf')\n",
    "        arg_max = 0\n",
    "        if not g.is_terminal(s):\n",
    "            #print('state:', s)\n",
    "            #print(' possible actions:', g.actions[s])\n",
    "            g.set_state(s)\n",
    "            # for each action, perform it, find the value of the next state. \n",
    "            # Save the one that provided the max value\n",
    "            for a in g.actions[s]:\n",
    "                states_prob = get_state_transition_prob(s, a, g)\n",
    "                r = g.move_without_randomness(a)\n",
    "                p = states_prob[g.current_state()]\n",
    "                val = p * (r + gamma * V[g.current_state()])\n",
    "                #print('  ', a, val)\n",
    "                if val > max_val:\n",
    "                    max_val = val\n",
    "                    argmax = a\n",
    "                g.undo_move(a)\n",
    "            #print(' action taken:', argmax)\n",
    "            policy[s] = argmax\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(g):\n",
    "    gamma = 0.9\n",
    "    states = g.all_states()\n",
    "    V = {}\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            if not g.is_terminal(s):\n",
    "                old_v = V[s]\n",
    "                max_a_val = float('-inf')\n",
    "                for a in g.actions[s]:\n",
    "                    val = 0\n",
    "                    # get the probabilities of each action to happen at that state\n",
    "                    states_prob = get_state_transition_prob(s, a, g)\n",
    "                    for s_prime in states_prob.keys():\n",
    "                        p = states_prob[s_prime]\n",
    "                        if s_prime in g.rewards.keys():\n",
    "                            r = g.rewards[s_prime]\n",
    "                        else:\n",
    "                            r = 0\n",
    "                        val += p * (r + gamma * V[s_prime])\n",
    "                    # Find the action that yields the maximum value\n",
    "                    if val > max_a_val:\n",
    "                        max_a_val = val\n",
    "                # Set the value of that state to be that maximum value (So that the value function is optimal already)\n",
    "                V[s] = max_a_val\n",
    "                delta = max(delta, abs(V[s] - old_v))\n",
    "        if delta < SMALL_ENOUGH:\n",
    "            break\n",
    "    \n",
    "    # Once we're done with the value function, find the corresponding policy greedily\n",
    "    policy = get_policy(V, g)\n",
    "        \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = standard_random_grid()\n",
    "policy, V = value_iteration(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "|  R  |  R  |  R  |     |\n",
      "-------------------------\n",
      "|  U  |     |  U  |     |\n",
      "-------------------------\n",
      "|  U  |  L  |  U  |  L  |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(policy, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "| 0.13| 0.27| 0.55| 0.00|\n",
      "-------------------------\n",
      "| 0.06| 0.00| 0.08| 0.00|\n",
      "-------------------------\n",
      "| 0.03| 0.02| 0.02|-0.16|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
